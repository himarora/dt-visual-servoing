{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # so we can do .to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is a method to solve for the optimal policy in Markov Decision Process by iteratively updating the value of each state. The algorithm for value iteration is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/value_iteration_algo.png\", width = 450>\n",
    "  <figcaption>Source: Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "In this example, we will use the cartpole environment from OpenAI gym. The states of this environment are cartpole position $x$, cartpole velocity $\\dot{x}$, pole angle $\\theta$, and pole angular velocity $\\dot{\\theta}$. \n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/cartpole.png\", width = 300>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "The episode is done when:\n",
    "- $x$ < -env.x_threshold or $x$ > env.x_threshold\n",
    "- $\\theta$ < -env.theta_threshold_radians or $\\theta$ > env.theta_threshold_radians\n",
    "\n",
    "The reward is 0 when the episode is done, otherwise the rewards is +1. In other words, we want the agent to keep the cartpole and pole within the range of desired states as long as possible.\n",
    "\n",
    "We're not actually going to run this until it converges because it will take too much time. However, it is still interesting to see how one can implement value iteration to solve this environment so we can understand and appreciate the challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedCartpoleEnv(gym.envs.classic_control.CartPoleEnv):\n",
    "    def reset(self, state=None):\n",
    "        \"\"\"\n",
    "        Modify reset function so we can teleport to any particular states if we want to.\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            self.state = state\n",
    "        else:\n",
    "            self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedCartpoleEnv()\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration requires us to compute the value of all $s \\in S$. Practically, we need to discretize the state and action space to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of possible states and range for each states\n",
    "n_x, x_min, x_max = 5, -env.x_threshold, env.x_threshold\n",
    "n_x_dot, x_dot_min, x_dot_max = 5, -8.0, 8.0 # actually from -inf to +inf\n",
    "n_theta, theta_min, theta_max = 5, -env.theta_threshold_radians, env.theta_threshold_radians\n",
    "n_theta_dot, theta_dot_min, theta_dot_max = 5, -10.0, 10.0 # actually from -inf to +inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States\n",
    "x_range = np.linspace(x_min, x_max, num = n_x)\n",
    "x_dot_range = np.linspace(x_dot_min, x_dot_max, num = n_x_dot)\n",
    "theta_range = np.linspace(theta_min, theta_max, num = n_theta)\n",
    "theta_dot_range = np.linspace(theta_dot_min, theta_dot_max, num = n_theta_dot)\n",
    "\n",
    "# Actions\n",
    "u_range = (0,1)\n",
    "\n",
    "# Create meshgrid\n",
    "x_v, x_dot_v, theta_v, theta_dot_v = np.meshgrid(x_range, x_dot_range, theta_range, theta_dot_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of list with all possible states\n",
    "states = []\n",
    "for i in range(n_x):\n",
    "    for j in range(n_x_dot):\n",
    "        for k in range(n_theta_dot):\n",
    "            for l in range(n_theta):\n",
    "                s = np.array([x_v[i,j,k,l], x_dot_v[i,j,k,l], theta_v[i,j,k,l], theta_dot_v[i,j,k,l]])\n",
    "                states.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    \"\"\"\n",
    "    Function to discretize state into the range we specified above.\n",
    "    NOTE: (x_range, x_dot_range, theta_range, theta_dot_range) are defined globally in this notebook.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = state[0]\n",
    "    x_dot = state[1]\n",
    "    theta = state[2]\n",
    "    theta_dot = state[3]\n",
    "    discretized_x = x_range[np.argmin(abs(x - x_range))] \n",
    "    discretized_x_dot = x_dot_range[np.argmin(abs(x_dot - x_dot_range))]\n",
    "    discretized_theta = theta_range[np.argmin(abs(theta - theta_range))]\n",
    "    discretized_theta_dot = theta_dot_range[np.argmin(abs(theta_dot - theta_dot_range))]\n",
    "    discretized_state = np.array([discretized_x, \n",
    "                                  discretized_x_dot, \n",
    "                                  discretized_theta, \n",
    "                                  discretized_theta_dot])\n",
    "    return discretized_state\n",
    "\n",
    "def get_state_index(states, discretized_state):\n",
    "    \"\"\"\n",
    "    Function to map the discretized_state onto its corresponding index in states.\n",
    "    \"\"\"\n",
    "    \n",
    "    diff = states - discretized_state\n",
    "    flags = np.any(diff, axis=1)\n",
    "    idx = np.where(flags==False)[0][0]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_threshold = 0.01\n",
    "delta = 100000\n",
    "gamma = 0.99\n",
    "deltas = []\n",
    "V = np.zeros(n_x * n_x_dot * n_theta * n_theta_dot) # initialize V(s) = 0 for all states\n",
    "\n",
    "num_iter = 0\n",
    "while delta > convergence_threshold:\n",
    "    delta = 0\n",
    "    for i in range(len(states)): # for each possible state\n",
    "        state = states[i]\n",
    "        v = V[i] # value of current state\n",
    "        V_max = -999999999\n",
    "        for u in u_range: # evaluate all possible actions (due to V(s) <- max_a ...)\n",
    "            env.reset(state = state) # make sure we are at state = state\n",
    "            next_state, r, done, _  = env.step(u) # apply u for one step\n",
    "            discretized_next_state = discretize_state(next_state)\n",
    "            next_state_index = get_state_index(states, discretized_next_state)\n",
    "            value = r + gamma * V[next_state_index]\n",
    "            if value > V_max:\n",
    "                V_max = value\n",
    "        V[i] = V_max # update value of current state\n",
    "        delta = max(delta, abs(v - V_max)) # update delta for convergence check\n",
    "    deltas.append(delta)\n",
    "    num_iter+=1\n",
    "    if num_iter % 10 == 0:\n",
    "        print('num_iter: ', num_iter, ' --- delta: ', delta)\n",
    "print('Value iteration convergef after %d iterations' % num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deltas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to determine how we choose an action based on $V(s)$. For example, we can do this greedily by doing:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')] \n",
    "$$\n",
    "\n",
    "So, let's create another lookup table for the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_policy(V):\n",
    "    \"\"\"\n",
    "    Function to return the optimal policy from a given V.\n",
    "    NOTE: (states, n_x, n_x_dot, n_theta_dot, n_theta, gamma) are defined globally in this notebook.\n",
    "    \"\"\"\n",
    "    policy = np.zeros(n_x * n_x_dot * n_theta_dot * n_theta)\n",
    "    for i in range(len(V)):\n",
    "        state = states[i]\n",
    "        V_max = -999999999\n",
    "        u_max = -999999999\n",
    "        for u in u_range: # evaluate all action\n",
    "            env.reset(state = state) # make sure we are at state = state\n",
    "            next_state, r, done, _  = env.step(u) # apply u for one step\n",
    "            discretized_next_state = discretize_state(next_state)\n",
    "            next_state_index = get_state_index(states, discretized_next_state)\n",
    "            value = r + gamma * V[next_state_index]\n",
    "            if value > V_max:\n",
    "                V_max = value\n",
    "                u_max = u\n",
    "        policy[i] = u_max\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_best_policy(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, policy):\n",
    "    \"\"\"\n",
    "    Function to get what action to take according to the policy.\n",
    "    NOTE: (states) is defined globally in this notebook.\n",
    "    \"\"\"\n",
    "    discretized_state = discretize_state(state)\n",
    "    state_index = get_state_index(states, discretized_state)\n",
    "    u = policy[state_index]\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = get_action(state, policy)\n",
    "    state, reward, done, _ = env.step(int(action))\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, we can identify several challenges:\n",
    "1. If discretization is not fine enough, then applying an action may not change the state at all.\n",
    "2. If discretization is too fine, then it will take a long time for the algorithm to converge.\n",
    "3. As the state and action spaces get bigger, the computation needed to do value iteration increases drastically.\n",
    "\n",
    "Now, let's take a look at REINFORCE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE is an on-policy (i.e., the model is trained exclusively on data generated using the current policy) policy gradient algorithm. Let our policy $\\pi$ to be parametrized by $\\theta$ (e.g., parameters of a neural network). Our goal is to find the parameter $\\theta$ that maximizes the expected return of trajectory $G(\\tau)$, where $\\tau$ denotes a trajectory obtained from following policy $\\pi_{\\theta}$. So the objective can be written as:\n",
    "\n",
    "$$\n",
    "J (\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [G(\\tau)]\n",
    "$$\n",
    "\n",
    "The policy gradient is then (we will skip the derivation of the policy gradient for now, but I highly encourage you to look at it):\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J (\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=0}^{T} G_{t}(\\tau) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)]\n",
    "$$\n",
    "\n",
    "Once we know $\\nabla_{\\theta} J (\\theta)$, we can update $\\theta$ with gradient ascent. In practice, we approximate the policy gradient via Monte-Carlo sampling by just considering a **single** trajectory, which gives us:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J (\\theta) \\approx \\sum_{t=0}^{T} G_{t}(\\tau) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)\n",
    "$$\n",
    "\n",
    "The REINFORCE algorithm is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/reinforce_algo.png\", width = 750>\n",
    "  <figcaption>Source: Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "For each iteration, we collect an episode trajectory following the current policy. At the end of each episode, we use this trajectory to get the return $G$ and log probability of action at each time step within the episode, which allows us to compute the policy gradient and update the parameters of the policy. After we update our network, we forget about the trajectory we just collected and collect a new one with the latest policy, and the processes are repeated.\n",
    "\n",
    "Let's take a look at the implementation. The code below is adapted from REINFORCE implementation example from PyTorch repo (source: https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the policy network\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = self.fc1(inp)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        logits = self.fc2(x)\n",
    "        out = F.softmax(logits, dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(REINFORCEAgent, self).__init__()\n",
    "        self.policy = PolicyNet(state_dim, action_dim).to(device)\n",
    "        self.policy.train()\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
    "        self.gamma = 0.99\n",
    "        self.log_interval = 10 # print progress per this many episodes\n",
    "    \n",
    "    def clean_memory(self):\n",
    "        del self.policy.rewards[:]\n",
    "        del self.policy.saved_log_probs[:]\n",
    "    \n",
    "    def select_action(self, input_state):\n",
    "        state = torch.from_numpy(input_state).float().unsqueeze(0)\n",
    "        action_probs = self.policy(state.to(device))\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        self.policy.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        G = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.policy.rewards[::-1]:\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-7) # normalized returns\n",
    "        for log_prob, G in zip(self.policy.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum() # a scalar\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.clean_memory() # empty self.rewards and self.saved_log_probs (on-policy!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4 # (x, x_dot, theta, theta_dot)\n",
    "action_dim = 2 # (0, 1)\n",
    "agent = REINFORCEAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "ep_rewards = [] # to store episode reward so we can plot it later\n",
    "for i in range(1, 10000):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for _ in range(1, 10000):\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        agent.policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    ep_rewards.append(ep_reward)\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    agent.update()\n",
    "    \n",
    "    if i % agent.log_interval == 0:\n",
    "        print('Episode %d \\t Running Reward: %.2f \\t Last Episode Reward: %d' % (i, running_reward, ep_reward))\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if running_reward > 100:\n",
    "        print('Solved: Episode %d \\t Running Reward: %.2f \\t Last Episode Reward: %d' \n",
    "              % (i, running_reward, ep_reward))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_rewards)\n",
    "plt.xlabel('episode #')\n",
    "plt.ylabel('episode rewards')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is an off-policy Q learning method. Unlike REINFORCE, DQN learns a Q networks (as opposed to policy) and can use trajectories that the agent has collected in the past to train the current Q network by storing them in a replay buffer. The learning objective in DQN is also different than in REINFORCE: DQN tries to minimize the **Temporal Difference (TD)** error (i.e., difference between prediction of Q values and the TD target). We are not going to talk about TD learning in detail here, but if you are especially interested to work with RL, I recommend you to look into TD learning and SARSA to have a better understanding about DQN. The TD target for DQN is defined as:\n",
    "\n",
    "$$\n",
    "Q_{target}(s,a) = r + \\gamma \\max_{a'}Q(s',a')\n",
    "$$\n",
    "\n",
    "The DQN algorithm is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/dqn_algo.png\", width = 600>\n",
    "  <figcaption>Source: Mnih et al. (2013). Playing Atari with Deep Reinforcement Learning.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Note that depending on the use case, we may or may not use the feature network $\\phi$ shown in the algorithm above. For this example, we know what the state is, so we do not need to differentiate between observations $x$ and states $s$.\n",
    "\n",
    "In practice, we often use another network that is a lagged copy of the Q-network to generate the TD target in order to stabilize training. We call this a **target network**. If you are interested to know more about this, I encourage you to read the paper :)\n",
    "\n",
    "Generally, with off-policy methods, one needs to be aware of memory usage since we are storing a lot of information in the replay buffer. This can be problematic especially when the state dimension is high (e.g., images).\n",
    "\n",
    "Let's now take a look at the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        # Use deque instead of list so we do not have to manually \n",
    "        # pop the buffer when it reaches max buffer size\n",
    "        self.buffer = deque(maxlen = buffer_size) \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function to push the data into buffer.\n",
    "        Input:\n",
    "            - state: state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - action: int\n",
    "            - reward: float\n",
    "            - next_state: next_state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - done: bool\n",
    "        \"\"\"\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function to sample a batch from replay buffer.\n",
    "        Input:\n",
    "            - batch_size: an int\n",
    "        \"\"\"\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            state = samples[i][0]\n",
    "            action = samples[i][1]\n",
    "            reward = samples[i][2]\n",
    "            next_state = samples[i][3]\n",
    "            done = samples[i][4]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = F.relu(self.dropout(self.fc1(inp)))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        \n",
    "        # DQN parameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = 1000\n",
    "        self.epsilon_init = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 200\n",
    "        self.gamma = 0.99\n",
    "        self.log_interval = 250 # print progress per this many episodes\n",
    "        self.target_update_freq = 10 # update target network per this many episodes\n",
    "        \n",
    "        # Models\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict()) # copy of q_network\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.q_network_optimizer = optim.Adam(self.q_network.parameters(), lr = 1e-2)\n",
    "        self.q_network.train()\n",
    "        self.target_network.eval() # We never train the network we use to generate the TD target!\n",
    "\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # [1, dim_space]\n",
    "        # Using epsilon greedy as our policy\n",
    "        if np.random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_value = self.q_network(state) # [1, action_space]\n",
    "                action  = q_value.max(1) # returns both the max values and max index\n",
    "                action = action[1].data[0] # [1] indicates we want the max index\n",
    "                action = action.item()\n",
    "        else: # With probability epsilon, select random action\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert ndarray to torch tensor\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        # Calculate TD error\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1) # [batch_size]\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach() # [0] indicates we want the max values (not the indices!), detach since this is the target # [batch_size]\n",
    "        td_target = rewards + self.gamma * next_q_values * (1. - dones) \n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "        \n",
    "        # Update model\n",
    "        self.q_network_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_network_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "running_reward = 10\n",
    "target_update_freq = 10 # update target network every X episode\n",
    "episode = 1 # indicate episode number\n",
    "\n",
    "state, ep_reward = env.reset(), 0\n",
    "for i in range(1, 10000):\n",
    "    \n",
    "    # Update epsilon and pick action\n",
    "    epsilon = agent.epsilon_end + (agent.epsilon_init - agent.epsilon_end) * np.exp(-1. * i / agent.epsilon_decay)\n",
    "    action = agent.get_action(state, epsilon)\n",
    "\n",
    "    # Take a step\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # Update replay buffer\n",
    "    agent.replay_buffer.update(state, action, reward, next_state, done)\n",
    "\n",
    "    # Once replay buffer size is larger than batch size, start training\n",
    "    if len(agent.replay_buffer.buffer) > batch_size:\n",
    "        agent.update(batch_size)\n",
    "\n",
    "    # Update episode reward and check for end episode\n",
    "    ep_reward += reward\n",
    "    if done: # If episode is done: update running reward, reset env, reset episode reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        episode += 1 # increment episode count (used for updating target network)\n",
    "        state, ep_reward = env.reset(), 0\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # Occasionally, update the target_network by copying the q_network\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "        \n",
    "    if i % agent.log_interval == 0:\n",
    "        print('Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if running_reward > 100:\n",
    "        print('Solved: Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
