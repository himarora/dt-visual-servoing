{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils.helpers import launch_env, wrap_env, view_results_ipython, change_exercise, seedall, force_done, evaluate_policy\n",
    "from utils.helpers import SteeringToWheelVelWrapper, ResizeWrapper, ImgWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Basics\n",
    "\n",
    "Reinforcement Learning, as we saw in lecture, is the idea of learning a _policy_ in order to maximize future (potentially discounted) rewards. Our policy, similar to the imitation learning network, maps raw image observations to wheel velocities, and at every timestep, receives a _reward_ from the environment. \n",
    "\n",
    "Rewards can be sparse (`1` if goal or task is completed, `0` otherwise) or dense; in general, dense rewards make it easier to learn policies, but as we'll see later in this exercise, defining the correct dense reward is an engineering challenge on its own.\n",
    "\n",
    "Today's reinforcement learning algorithms are often a mix between _value-based_ and _policy-gradient_ algorithms, instances of what is called an _actor-critic_ formulation. Actor-critic methods have had a lot of research done on them in recent years (especially within in the deep reinforcement learning era), and later in this exercise, we shall also rediscover the formulation's original problems and different methods currently used to stabilize learning.\n",
    "\n",
    "We begin by defining two networks, an `Actor` and `Critic`; in this exercise, we'll be using a deep RL algorithm titled _Deep Deterministic Policy Gradients_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        # TODO: You'll need to change this!\n",
    "        flat_size = 0\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, stride=2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "        self.lin1 = nn.Linear(flat_size, 100)\n",
    "        self.lin2 = nn.Linear(100, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.relu(self.conv1(x)))\n",
    "        x = self.bn2(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.lr(self.lin1(x))\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = self.max_action * self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, action_dim, max_action):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # TODO: You'll need to change this!\n",
    "        flat_size = 0\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, stride=2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "        self.lin1 = nn.Linear(flat_size + action_dim, 100)\n",
    "        self.lin2 = nn.Linear(100, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        x = self.bn1(self.relu(self.conv1(obs)))\n",
    "        x = self.bn2(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat([x, action], 1)\n",
    "        x = self.lr(self.lin1(x))\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = self.max_action * self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Engineering\n",
    "\n",
    "In this part of the exercise, we will experiment with the reward formulation. Given the same model, we'll see how the effect of various reward functions changes the final policy trained. \n",
    "\n",
    "In the section below, we'll take a look at the reward function implemented in `gym-duckietown` with a slightly modified training loop. Traditionally, we `reset()` the environment to start an episode, and then `step()` the environment forward for a set amount of time, executing a new action. If this sounds a bit odd, especially for roboticists, you're right - in real robotics, most code runs asynchronously. As a result, although `gym-duckietown` runs locally by stopping the environment, the `AIDO` submissions will run asynchronously, executing the same action until a new one is received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_reward(env):\n",
    "    # Compute the collision avoidance penalty\n",
    "    pos, angle, speed = env.cur_pos, env.cur_angle, env.speed\n",
    "    col_penalty = env.proximity_penalty2(pos, angle)\n",
    "\n",
    "    # Get the position relative to the right lane tangent\n",
    "    try:\n",
    "        lp = env.get_lane_pos2(pos, angle)\n",
    "    except NotInLane:\n",
    "        reward = 40 * col_penalty\n",
    "    else:\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = (\n",
    "                +1.0 * speed * lp.dot_dir +\n",
    "                -10 * np.abs(lp.dist) +\n",
    "                +40 * col_penalty\n",
    "        )\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepisodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_env = launch_env()\n",
    "local_env = wrap_env(local_env)\n",
    "local_env = ResizeWrapper(local_env)\n",
    "local_env = ImgWrapper(local_env)\n",
    "\n",
    "for _ in range(nepisodes):\n",
    "    done = False\n",
    "    obs = local_env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        obs, r, done, info = local_env.step(np.random.random(2))\n",
    "        new_r = updated_reward(local_env)\n",
    "        print(r, new_r)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_results_ipython(local_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 0: After understanding the above computed reward, experiment with the constants for each component. What type of behavior does the above reward function penalize? Is this good or bad in context of autonomous driving? Name some other issues that can arise with single-objective optimization. In addition, give three sets of constants and explain qualitatively what types of behavior each penalizes or rewards (note, you may want to use a different action policy than random)**. Place the answers to the above in `reinforcement-learning-answers.txt`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reinforcement Learning Learning Code\n",
    "\n",
    "Below we'll see a relatively naive implementation of the actor-critic training loop, which proceeds as follows: the critic is tasked with a supervised learning problem of fitting rewards acquired by the agent. Then, the policy, using policy gradients, maximizes the return according to the critic's estimate, rather than using Monte-Carlo updates.\n",
    "\n",
    "Below, we see an implementation of `DDPGAgent`, a class which handles the networks and training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DDPGAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action=1.0):\n",
    "        super(DDPGAgent, self).__init__()\n",
    "\n",
    "        self.actor = Actor(action_dim, max_action).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=5e-2)\n",
    "        \n",
    "        self.critic = CriticCNN(action_dim).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=5e-2)\n",
    "\n",
    "    def predict(self, state):\n",
    "        assert state.shape[0] == 3\n",
    "        state = torch.FloatTensor(np.expand_dims(state, axis=0)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=64, discount=0.99):\n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Sample replay buffer\n",
    "            sample = replay_buffer.sample(batch_size, flat=self.flat)\n",
    "            state = torch.FloatTensor(sample[\"state\"]).to(device)\n",
    "            action = torch.FloatTensor(sample[\"action\"]).to(device)\n",
    "            next_state = torch.FloatTensor(sample[\"next_state\"]).to(device)\n",
    "            done = torch.FloatTensor(1 - sample[\"done\"]).to(device)\n",
    "            reward = torch.FloatTensor(sample[\"reward\"]).to(device)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q = self.critic(next_state, self.actor(next_state))\n",
    "            \n",
    "            # TODO: - no detach is a subtle, but important bug!\n",
    "            target_Q = reward + (done * discount * target_Q)\n",
    "\n",
    "            # Get current Q estimate\n",
    "            current_Q = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '{}/{}_actor.pth'.format(directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '{}/{}_critic.pth'.format(directory, filename))\n",
    "\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('{}/{}_actor.pth'.format(directory, filename), map_location=device))\n",
    "        self.critic.load_state_dict(torch.load('{}/{}_critic.pth'.format(directory, filename), map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the training loop needs a `replay_buffer` object. In value-based and actor-critic methods in deep reinforcement learning, the use of a replay buffer is crucial. In the following sections, you'll explore why this is the case, and some other stabilization techniques that are needed in order to get the above code to work. Below, you can find an implementation of the replay buffer, as well the training loop that we use to train DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    # Expects tuples of (state, next_state, action, reward, done)\n",
    "    def add(self, state, next_state, action, reward, done):\n",
    "        if len(self.storage) < self.max_size:\n",
    "            self.storage.append((state, next_state, action, reward, done))\n",
    "        else:\n",
    "            # Remove random element in the memory beforea adding a new one\n",
    "            self.storage.pop(random.randrange(len(self.storage)))\n",
    "            self.storage.append((state, next_state, action, reward, done))\n",
    "\n",
    "\n",
    "    def sample(self, batch_size=100, flat=True):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, next_states, actions, rewards, dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "\n",
    "            if flat:\n",
    "                states.append(np.array(state, copy=False).flatten())\n",
    "                next_states.append(np.array(next_state, copy=False).flatten())\n",
    "            else:\n",
    "                states.append(np.array(state, copy=False))\n",
    "                next_states.append(np.array(next_state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(np.array(reward, copy=False))\n",
    "            dones.append(np.array(done, copy=False))\n",
    "\n",
    "        # state_sample, action_sample, next_state_sample, reward_sample, done_sample\n",
    "        return {\n",
    "            \"state\": np.stack(states),\n",
    "            \"next_state\": np.stack(next_states),\n",
    "            \"action\": np.stack(actions),\n",
    "            \"reward\": np.stack(rewards).reshape(-1,1),\n",
    "            \"done\": np.stack(dones).reshape(-1,1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 123\n",
    "max_timesteps = 500 \n",
    "batch_size = 64\n",
    "discount = 0.99\n",
    "eval_freq = 5e3\n",
    "file_name = 'dt-class-rl'\n",
    "start_timesteps = 1e4\n",
    "expl_noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_env = launch_env()\n",
    "local_env = wrap_env(local_env)\n",
    "local_env = ResizeWrapper(local_env)\n",
    "local_env = ImgWrapper(local_env)\n",
    "\n",
    "if not os.path.exists(\"./pytorch_models\"):\n",
    "    os.makedirs(\"./pytorch_models\")\n",
    "\n",
    "# Set seeds\n",
    "seedall(seed_)\n",
    "\n",
    "state_dim = local_env.observation_space.shape\n",
    "action_dim = local_env.action_space.shape[0]\n",
    "max_action = float(local_env.action_space.high[0])\n",
    "\n",
    "# Initialize policy\n",
    "policy = DDPGAgent(state_dim, action_dim, max_action)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# Evaluate untrained policy\n",
    "evaluations= [evaluate_policy(local_env, policy)]\n",
    "\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "episode_reward = None\n",
    "env_counter = 0\n",
    "while total_timesteps < max_timesteps:\n",
    "    if done:\n",
    "        if total_timesteps != 0:\n",
    "            print((\"Total T: %d Episode Num: %d Episode T: %d Reward: %f\") % (\n",
    "                total_timesteps, episode_num, episode_timesteps, episode_reward))\n",
    "            policy.train(replay_buffer, episode_timesteps, batch_size, discount)\n",
    "\n",
    "        # Evaluate episode\n",
    "        if timesteps_since_eval >= eval_freq:\n",
    "            timesteps_since_eval %= eval_freq\n",
    "            evaluations.append(evaluate_policy(local_env, policy))\n",
    "\n",
    "            policy.save(file_name, directory=\"./pytorch_models\")\n",
    "            np.savez(\"./pytorch_models/{}.npz\".format(file_name),evaluations)\n",
    "\n",
    "        # Reset environment\n",
    "        env_counter += 1\n",
    "        obs = local_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "\n",
    "    # Select action randomly or according to policy\n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = local_env.action_space.sample()\n",
    "    else:\n",
    "        action = policy.predict(np.array(obs))\n",
    "        if expl_noise != 0:\n",
    "            action = (action + np.random.normal(\n",
    "                0,\n",
    "                expl_noise,\n",
    "                size=local_env.action_space.shape[0])\n",
    "            ).clip(-1, +1)\n",
    "\n",
    "    # Perform action\n",
    "    new_obs, reward, done, _ = local_env.step(action)\n",
    "\n",
    "    if episode_timesteps >= env_timesteps:\n",
    "        done = True\n",
    "\n",
    "    done_bool = 0 if episode_timesteps + 1 == env_timesteps else float(done)\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(obs, new_obs, action, reward, done_bool)\n",
    "\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n",
    "# Final evaluation\n",
    "evaluations.append(evaluate_policy(local_env, policy))\n",
    "\n",
    "if args.save_models:\n",
    "    policy.save(file_name, directory=\"./pytorch_models\")\n",
    "np.savez(\"./pytorch_models/{}.npz\".format(file_name),evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stabilizing DDPG\n",
    "\n",
    "As you may notice, the above model performs poorly or doesn't converge. Your job is to improve it; first in the notebook, later in the AIDO submission. This last part of the assignment consists of four sections:\n",
    "\n",
    "**1. There are subtle, but important, bugs that have been introduced into the code above. Your job is to find them, and explain them in your `reinforcement-learning-answers.txt`. You'll want to reread the original [DQN](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning) and [DDPG](https://arxiv.org/abs/1509.02971) papers in order to better understand the issue, but by answering the following subquestions (*please put the answers to these in the submission for full credit*), you'll be on the right track:**\n",
    "\n",
    "   a) Read some literature on actor-critic methods, including the original [actor-critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) paper. What is an issue that you see related to *non-stationarity*? Define what _non-stationarity_ means in the context of machine learning and how it relates to actor-critic methods. In addition, give some hypotheses on why reinforcement learning is much more difficult (from an optimization perspective) than supervised learning, and how the answer to the previous question and this one are related.\n",
    "\n",
    "   b) What role does the replay buffer play in off-policy reinforcement learning? It's most important parameter is `max_size` - how does changing this value (answer for both increasing and decreasing trends) qualitatively affect the training of the algorithm?\n",
    "\n",
    "   c) **Challenge Question:** Briefly, explain how automatic differentiation works. In addition, expand on the difference between a single-element tensor (that `requires_grad`) and a scalar value as it relates to automatic differentiation; when do we want to backpropogate through a single-element tensor, and when do we not? Take a close look at the code and how losses are being backpropogated. On paper or your favorite drawing software, draw out the actor-critic architecture *as described in the code*, and label how the actor and critic losses are backpropogated. On your diagram, highlight the particular loss that will cause issues with the above code, and fix it.\n",
    "   \n",
    "For the next section, please pick **either** the theoretical or the practical pathway. If you don't have access to the necessary compute, for the exercise, please do the theoretical portion. \n",
    "   \n",
    "_Theoretical Component_ \n",
    "\n",
    "**2. We discussed a case study of DQN in class. The original authors used quite a few tricks to get this to work. Detail some of the following, and explain what problem they solve in training the DQN:**\n",
    "\n",
    "a) Target Networks\n",
    "\n",
    "b) Annealed Learning Rates\n",
    "\n",
    "c) Replay Buffer\n",
    "\n",
    "d) Random Exploration Period\n",
    "\n",
    "e) Preprocessing the Image\n",
    "\n",
    "\n",
    "**3. Read about either [TD3](https://arxiv.org/abs/1802.09477) or [Soft Actor Critic](https://arxiv.org/abs/1801.01290); for your choice, summarize what problems they are addressing with the standard actor-critic formulation, and how they solve them**\n",
    "\n",
    "\n",
    "_Practical Component_ \n",
    "\n",
    "**2. [Optional - if you have access to compute] Using your analysis from the reward engineering ablation, train two agents (after you've found the bugs in DDPG) - one with the standard, `gym-duckietown` reward, and another with the parameters of your choosing. Report each set of parameters, and describe qualitatively what type of behavior the agent produces.**\n",
    "\n",
    "If you don't have the resources to actually train these agents, instead describe what types of behaviors each reward function might prioritize.\n",
    "\n",
    "**3. [Optional - if you have access to compute] Using the instructions [here](http://docs.duckietown.org/DT19/AIDO/out/embodied_rl.html), use the saved policy files from this notebook and submit using the template submission provided through the AIDO submission. Report your best submission number (i.e the one you'd like to be graded) in `reinforcement-learning-answers.txt`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
