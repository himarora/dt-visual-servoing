{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils.helpers import launch_env, wrap_env, view_results_ipython\n",
    "from simulation.gym_duckietown.wrappers import SteeringToWheelVelWrapper\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control\n",
    "\n",
    "In robotics, control is often considered a discipline in its own right. Simply, control theory allows us to map from sensory data (or, some sort of intermediate representation) to actuation commands - these commands can help drive our Duckiebot, fly our planes, and even land our rockets.\n",
    "\n",
    "For those coming from a machine learning background (or those with familiarity with reinforcement learning and imitation learning), you may have noticed a difference between the control covered in class and the controllers, or *policies*, that you may normally train. In robotics, an important part of control is feedback; Feedback allows our controller to self-regulate and tune, and is an important part of the control process. \n",
    "\n",
    "In this exercise, we will cover two controllers: a simple, PD controller, and a slightly more complicated pure-pursuit controller in Part 2. \n",
    "\n",
    "\n",
    "## Part 1: PID\n",
    "\n",
    "A PID controller, named for its components of **Proportional**, **Integral**, and **Derivative**, is a simple yet robust mechanism. This generality has lead to its widespread use across multiple disciplines, and is what our exercise will start with. More eloquently (from Wikipedia): *A PID controller continuously calculates an error value e(t) as the difference between a desired setpoint (SP) and a measured process variable (PV) and applies a correction based on proportional, integral, and derivative terms (denoted P, I, and D respectively), hence the name.*\n",
    "\n",
    "*In practical terms it automatically applies accurate and responsive correction to a control function.*\n",
    "\n",
    "In our exercise, we will only implement the **P**ropotional and (an approximation to the) **D**erivative control corrections.\n",
    "\n",
    "Before we begin with the practical portion, the **first part of the exercise** consists of the following:\n",
    "\n",
    "*Explain, qualitatively, the difference between the three terms. For the following types of controllers, explain the error terms involved in the correction, and what types of applications they are commonly used for. If these controllers do not exist (or cannot), explain why.*\n",
    "\n",
    "1. P\n",
    "2. PI\n",
    "3. PD\n",
    "4. ID\n",
    "5. PID\n",
    "\n",
    "Put your answers inside of the zip file, inside of a file named **03-control.txt**.\n",
    "\n",
    "***\n",
    "\n",
    "As we continue towards the practical portion of the first exercise, we will be tasked with setting up a controller that keeps the Duckiebot on the road. \n",
    "\n",
    "In the kinematics exercise, we saw how to translate linear and angular velocity commands into wheel rate. After understanding the conversion, we still need to answer a more important question: *what* commands should we use to control the Duckiebot?\n",
    "\n",
    "Since we're using a simulation, we are able to access the ground-truth values of two variables that are crucial to help our Duckiebot stay in the lane: $d$, the distance to the center of the lane, and $\\omega$, the angle with respect to the lane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your file `notebooks/code/exercise_03_control/controller.py`, complete function `angle_control_commands`. Update `omega` (angular speed) to achieve your goal while keeping `v` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.code.exercise_03_control.controller import Controller\n",
    "\n",
    "# Setting up the environment\n",
    "local_env = launch_env()\n",
    "local_env = SteeringToWheelVelWrapper(wrap_env(local_env))\n",
    "local_env.reset()\n",
    "local_env.robot_speed = 0.5\n",
    "total_reward = 0        \n",
    "controller = Controller()\n",
    "    \n",
    "# Starting to drive\n",
    "for _ in range(1000):\n",
    "    # Getting the pose\n",
    "    lane_pose = local_env.get_lane_pos2(local_env.cur_pos, local_env.cur_angle)\n",
    "    dist = lane_pose.dist        # Distance to lane center. Left is negative, right is positive.\n",
    "    angle = lane_pose.angle_rad  # Angle from straight, in radians. Left is negative, right is positive.\n",
    "    \n",
    "    # Control\n",
    "    v = 0.5  # For now, keep linear velocity constant                                                \n",
    "    omega = controller.angle_control_commands(dist, angle)\n",
    "    commands = np.array([v, omega])\n",
    "    \n",
    "    # Step\n",
    "    _, r, d, _ = local_env.step(commands)\n",
    "    total_reward += r\n",
    "    \n",
    "    if d: \n",
    "        print(\"Duckiebot crashed.\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "local_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how well you are doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your score: {}\".format(total_reward))\n",
    "view_results_ipython(local_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within **03-control.txt**, answer the following question:\n",
    "\n",
    "*Describe the method you used. What kind of controller is it?*\n",
    "\n",
    "Then, from the list of popular variants of PID controllers, implement a second one. Answer the above question again, and then discuss the differences (both in terms of algorithm and performance) between the two. Do you see any qualitative differences in the driving? Why or why not? Put your answers in **03-control.txt**.\n",
    "\n",
    "Lastly, for both implementation, plot the **cross-track** error and the **angle error**, and submit the plot(s) (depending on if you decide to separate the metrics on two plots) in the zip file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pure Pursuit\n",
    "\n",
    "Within **03-control.txt**, answer the following question:\n",
    "\n",
    "*While robust, general, and simple, PID controllers also have limitations. What are some failure cases of PID controllers?*\n",
    " \n",
    "In this section, we'll be looking into a type of controller known as a *Pure Pursuit* (PP) controller. A PP controller is not explicitly a reactive controller, but actually requires a reference trajectory that the controller follows. When we input the pose, velocity, and current waypoint, the controller outputs control commands that are used to steer the robot.\n",
    "\n",
    "Besides robot-specific parameters (which need to be set only once), a PP controller has a single, tunable parameter: *look-ahead* distance. The look-ahead distance is how far along the current path (more on how we do this next section) the robot should _look ahead_ from to compute the angular velocity commands. For simplicity, in our example, the velocity will be fixed.\n",
    "\n",
    "Before we move on to the code, write the answer to the following in **03-control.txt**:\n",
    "\n",
    "*Recall the lecture notes about PP controllers. How does the look-ahead distance affect performance? What does a small look-ahead distance prioritize? What about a larger one?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "As we start the last practical portion of the exercise, we recall a requirement of the PP controller: a reference trajectory. How can we get this from the simulator?\n",
    "\n",
    "The answer lies inside of the `simulator.py` code, available [here](https://github.com/duckietown/gym-duckietown/blob/master/gym_duckietown/simulator.py). In `gym-duckietown`, we load every tile with a *Bezier* curve, which allows us to calculate a *reward* (useful in reinforcement learning, which we will see later). Each Bezier curve was designed to perfectly fit the right lane of each tile.\n",
    "\n",
    "`gym-duckietown` provides a few helper functions that we will make use of. Inside of `controller.py:Controller:pure_pursuit`, you will notice a call to `closest_curve_point`, which returns the closest point on the reference trajectory. \n",
    "\n",
    "While we have a boilerplate function written, it still needs work. In particular, the main things that need your attention are:\n",
    "\n",
    "- `follow_point` needs to be a function of a few more items, not just the closest point.\n",
    "- Once you have the correct waypoint, you need to calculate the $\\omega$ that gets you there. You can also change $v$ to go as fast as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.code.exercise_03_control.controller import Controller\n",
    "\n",
    "# Reset environment\n",
    "local_env = launch_env()\n",
    "local_env = SteeringToWheelVelWrapper(wrap_env(local_env))\n",
    "local_env.reset()\n",
    "local_env.robot_speed = 0.5\n",
    "controller = Controller()\n",
    "total_reward = 0             \n",
    "    \n",
    "# Starting to drive\n",
    "for _ in range(1000):\n",
    "   \n",
    "    # Getting the global pose\n",
    "    pos = local_env.cur_pos\n",
    "    angle = local_env.cur_angle\n",
    "    \n",
    "    # Control\n",
    "    v, omega = controller.pure_pursuit(local_env, pos, angle)\n",
    "    commands = np.array([v, omega])\n",
    "    \n",
    "    # Step\n",
    "    _, r, d, _ = local_env.step(commands)\n",
    "    total_reward += r\n",
    "    \n",
    "    if d: \n",
    "        print(\"Duckiebot crashed.\")\n",
    "        break\n",
    "        \n",
    "local_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see qualitatively how you are doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your score: {}\".format(total_reward))\n",
    "view_results_ipython(local_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this portion of the exercise, submit one plot with **cross-track error**, **angle error**, **commanded linear velocity** and **commanded angular velocity** vs. time. Lastly, experiment with the `lookup_distance` hyperparameter; does the lowering / raising this value match your hypothesis from earlier in this exercise?\n",
    "\n",
    "Please submit the plot, `controller.py`, and `control.txt` inside of the zip file, and upload it to the Software submission link posted on Piazza."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
